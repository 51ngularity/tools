{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b3e5205",
   "metadata": {},
   "source": [
    "## Original-Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ed35b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Listing 1                                                    ###\n",
    "### implementiert ein einfaches künstliches neuronales Netzwerk. ###\n",
    "### Feedforward-Netzwerk, das durch Backpropagation              ###\n",
    "import math\n",
    "import csv\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "\n",
    "# Sigmoidfunktion als Aktivierungsfunktion\n",
    "def sigmoid(x):\n",
    "    try:\n",
    "        return 1 / (1 + math.exp(-x))\n",
    "    except OverflowError:\n",
    "        return 0\n",
    "\n",
    "# Künstliches neuronales Netzwerk\n",
    "class NeuralNetwork:\n",
    "\n",
    "    # Attribute:\n",
    "    # - Anzahl Neuronen der Eingabeschicht\n",
    "    # - Anzahl Neuronen der versteckten Schicht\n",
    "    # - Anzahl Neuronen der Ausgabeschicht\n",
    "    # - Lernrate (benannt)\n",
    "    def __init__(self, i_neurons, h_neurons, o_neurons, learning_rate = 0.1):\n",
    "        # Grundattribute initialisieren\n",
    "        self.input_neurons = i_neurons\n",
    "        self.hidden_neurons = h_neurons\n",
    "        self.output_neurons = o_neurons\n",
    "        self.learning_rate = learning_rate\n",
    "        self.categories = []\n",
    "\n",
    "        # Gewichte als Zufallswerte initialisieren\n",
    "        self.input_to_hidden = np.random.rand(\n",
    "            self.hidden_neurons, self.input_neurons\n",
    "        ) - 0.5\n",
    "        #print(type(self.input_to_hidden))\n",
    "        #print('I to H :',self.input_to_hidden)      # 4 * 12 Matrix\n",
    "        self.hidden_to_output = np.random.rand(\n",
    "            self.output_neurons, self.hidden_neurons\n",
    "        ) - 0.5\n",
    "        #print('H to O :',self.hidden_to_output)     # 12 * 3 Matrix\n",
    "        # Aktivierungsfunktion für NumPy-Arrays\n",
    "        self.activation = np.vectorize(sigmoid)\n",
    "\n",
    "    # Daten vorbereiten\n",
    "    # Attribute:\n",
    "    # data       - Daten als zweidimensionale Liste\n",
    "    # test_ratio - Anteil, der als Testdaten abgespalten werden soll\n",
    "    # last       - Kategorie in der letzten Spalte? Sonst in der ersten\n",
    "    def prepare(self, data, test_ratio=0.1, last=True):\n",
    "        if last:\n",
    "            x = [line[0:-1] for line in data]  # Daten/Features\n",
    "            y = [line[-1] for line in data]    # Kategorie aus letzter Spalte\n",
    "        else:\n",
    "            x = [line[1:] for line in data]    # Daten/Features\n",
    "            y = [line[0] for line in data]     # Kategorie aus erster Spalte\n",
    "        # Feature-Skalierung (x)\n",
    "        columns = np.array(x).transpose()      # array erzeugen; Spalte-Zeile tauschen\n",
    "        #print(np.array(x))\n",
    "        #print('########\\n', columns)\n",
    "        x_scaled = []\n",
    "        for column in columns:                 # Normalisieren: max. Wert = 1\n",
    "            #print(min(column), max(column))   #                min. Wert = 0 \n",
    "            if min(column) == max(column):\n",
    "                column = np.zeros(len(column))\n",
    "            else:\n",
    "                column = (column - min(column)) / (max(column) - min(column))\n",
    "            x_scaled.append(column)\n",
    "        x = np.array(x_scaled).transpose()\n",
    "        #print(x)\n",
    "        # Kategorien extrahieren und als Attribut speichern\n",
    "        y_values = list(set(y))                # Unique durch set()\n",
    "        self.categories = y_values             # Objektvariable setzen\n",
    "        # Verteilung auf Ausgabeneuronen (y)\n",
    "        y_spread = []\n",
    "        for y_i in y:\n",
    "            current = np.zeros(len(y_values))  # array 1D, Länge y-Werte mit Nullen\n",
    "            current[y_values.index(y_i)] = 1   # Setze Index\n",
    "            y_spread.append(current)           # Array Eintrag in Liste  \n",
    "        y_out = np.array(y_spread)\n",
    "        #print(y_out)\n",
    "        separator = int(test_ratio * len(x))   # Trenner für Testdaten\n",
    "        return x[:separator], y[:separator], x[separator:], y_out[separator:]\n",
    "\n",
    "    # Ein einzelner Trainingsdurchgang\n",
    "    # Attribute:\n",
    "    # - Eingabedaten als zweidimensionale Liste/Array\n",
    "    # - Zieldaten als auf Ausgabeneuronen verteilte Liste/Array\n",
    "    def train(self, inputs, targets):\n",
    "        # Daten ins richtige Format bringen\n",
    "        inputs = np.array(inputs, ndmin = 2).transpose()\n",
    "        targets = np.array(targets, ndmin = 2).transpose()\n",
    "        # Matrixmultiplikation: Gewichte versteckte Schicht * Eingabe\n",
    "        hidden_in = np.dot(self.input_to_hidden, inputs)\n",
    "        # Aktivierungsfunktion anwenden\n",
    "        hidden_out = self.activation(hidden_in)\n",
    "        # Matrixmultiplikation: Gewichte Ausgabeschicht * Ergebnis versteckt\n",
    "        output_in = np.dot(self.hidden_to_output, hidden_out)\n",
    "        # Aktivierungsfunktion anwenden\n",
    "        output_out = self.activation(output_in)\n",
    "        # Die Fehler berechnen\n",
    "        output_diff = targets - output_out\n",
    "        hidden_diff = np.dot(self.hidden_to_output.transpose(), output_diff)\n",
    "        # Die Gewichte mit Lernrate * Fehler anpassen\n",
    "        self.hidden_to_output += (\n",
    "            self.learning_rate *\n",
    "            np.dot(\n",
    "                (output_diff * output_out * (1.0 - output_out)),\n",
    "                hidden_out.transpose()\n",
    "            )\n",
    "        )\n",
    "        self.input_to_hidden += (\n",
    "            self.learning_rate *\n",
    "            np.dot(\n",
    "                (hidden_diff * hidden_out * (1.0 * hidden_out)),\n",
    "                inputs.transpose()\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Vorhersage für eine Reihe von Testdaten\n",
    "    # Attribute:\n",
    "    # - Eingabedaten als zweidimensionale Liste/Array\n",
    "    # - Vergleichsdaten (benannt, optional)\n",
    "    def predict(self, inputs, targets = None):\n",
    "        # Dieselben Schritte wie in train()\n",
    "        inputs = np.array(inputs, ndmin = 2).transpose()\n",
    "        hidden_in = np.dot(self.input_to_hidden, inputs)\n",
    "        hidden_out = self.activation(hidden_in)\n",
    "        output_in = np.dot(self.hidden_to_output, hidden_out)\n",
    "        output_out = self.activation(output_in)\n",
    "        # Ausgabewerte den Kategorien zuweisen\n",
    "        outputs = output_out.transpose()\n",
    "        result = []\n",
    "        for output in outputs:\n",
    "            result.append(\n",
    "                self.categories[list(output).index(max(output))]\n",
    "            )\n",
    "        # Wenn keine Zielwerte vorhanden, Ergebnisliste zurückgeben\n",
    "        #print('Ergebnis:',result)\n",
    "        #print('Vergleich:', targets)\n",
    "        if targets is None:\n",
    "            return result\n",
    "        # Ansonsten vergleichen und korrekte Vorhersagen zählen\n",
    "        correct = 0\n",
    "        for res, pred in zip(targets, result):\n",
    "            if res == pred:\n",
    "                correct += 1\n",
    "        percent = correct / len(result) * 100\n",
    "        return correct, percent\n",
    "\n",
    "\n",
    "# Hauptprogramm\n",
    "if __name__ == '__main__':\n",
    "    with open('iris_nn.csv', 'r') as iris_file:\n",
    "        reader = csv.reader(iris_file, quoting=csv.QUOTE_NONNUMERIC)\n",
    "        irises = list(reader)\n",
    "    shuffle(irises)\n",
    "    network = NeuralNetwork(4, 12, 3, learning_rate = 0.2)\n",
    "    x_test, y_test, x_train, y_train = network.prepare(irises, test_ratio=0.2)\n",
    "    #print(x_train,'\\n',y_train)\n",
    "    #print(x_test,'\\n',y_test)\n",
    "    for i in range(200):\n",
    "        network.train(x_train, y_train)\n",
    "    correct, percent = network.predict(x_test, targets = y_test)\n",
    "    print(f\"{correct} korrekte Vorhersagen ({percent}%).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e1baf6",
   "metadata": {},
   "source": [
    "## Hinzufügen einer variablen Anzahl von hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e67ca4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Listing 1                                                    ###\n",
    "### implementiert ein einfaches künstliches neuronales Netzwerk. ###\n",
    "### Feedforward-Netzwerk, das durch Backpropagation              ###\n",
    "import math\n",
    "import csv\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "\n",
    "# Sigmoidfunktion als Aktivierungsfunktion\n",
    "def sigmoid(x):\n",
    "    try:\n",
    "        return 1 / (1 + math.exp(-x))\n",
    "    except OverflowError:\n",
    "        return 0\n",
    "\n",
    "# Künstliches neuronales Netzwerk\n",
    "class NeuralNetwork:\n",
    "\n",
    "    # Attribute:\n",
    "    # - Anzahl Neuronen der Eingabeschicht\n",
    "    # - Anzahl Neuronen der versteckten Schicht\n",
    "    # - Anzahl Neuronen der Ausgabeschicht\n",
    "    # - Lernrate (benannt)\n",
    "    def __init__(self, i_neurons, h_neurons, o_neurons, learning_rate = 0.1):\n",
    "        # Grundattribute initialisieren\n",
    "        self.input_neurons = i_neurons\n",
    "        \n",
    "#         h_neurons is now passed as a list with multiple values\n",
    "#         indicating the number of neurons inside the different hidden layers\n",
    "        self.hidden_neurons = h_neurons\n",
    "        self.output_neurons = o_neurons\n",
    "        self.learning_rate = learning_rate\n",
    "        self.categories = []\n",
    "\n",
    "        # Gewichte als Zufallswerte initialisieren\n",
    "        self.weights = {}\n",
    "        self.weights[\"input_to_hidden1\"] = np.random.rand(\n",
    "            self.hidden_neurons[0], self.input_neurons\n",
    "        ) - 0.5\n",
    "        \n",
    "        if len(self.hidden_neurons) > 1:\n",
    "            for i, _ in enumerate(self.hidden_neurons[1:]):\n",
    "                self.weights[f\"hidden{i+1}_to_hidden{i+2}\"] = np.random.rand(\n",
    "                    self.hidden_neurons[i+1] , self.hidden_neurons[i]\n",
    "                ) - 0.5\n",
    "\n",
    "\n",
    "        self.weights[f\"hidden{len(self.hidden_neurons)}_to_output\"] = np.random.rand(\n",
    "            self.output_neurons, self.hidden_neurons[-1]\n",
    "        ) - 0.5\n",
    "\n",
    "        # Aktivierungsfunktion für NumPy-Arrays\n",
    "        self.activation = np.vectorize(sigmoid)\n",
    "\n",
    "    # Daten vorbereiten\n",
    "    # Attribute:\n",
    "    # data       - Daten als zweidimensionale Liste\n",
    "    # test_ratio - Anteil, der als Testdaten abgespalten werden soll\n",
    "    # last       - Kategorie in der letzten Spalte? Sonst in der ersten\n",
    "    def prepare(self, data, test_ratio=0.1, last=True):\n",
    "        if last:\n",
    "            x = [line[0:-1] for line in data]  # Daten/Features\n",
    "            y = [line[-1] for line in data]    # Kategorie aus letzter Spalte\n",
    "        else:\n",
    "            x = [line[1:] for line in data]    # Daten/Features\n",
    "            y = [line[0] for line in data]     # Kategorie aus erster Spalte\n",
    "        # Feature-Skalierung (x)\n",
    "        columns = np.array(x).transpose()      # array erzeugen; Spalte-Zeile tauschen\n",
    "        #print(np.array(x))\n",
    "        #print('########\\n', columns)\n",
    "        x_scaled = []\n",
    "        for column in columns:                 # Normalisieren: max. Wert = 1\n",
    "            #print(min(column), max(column))   #                min. Wert = 0 \n",
    "            if min(column) == max(column):\n",
    "                column = np.zeros(len(column))\n",
    "            else:\n",
    "                column = (column - min(column)) / (max(column) - min(column))\n",
    "            x_scaled.append(column)\n",
    "        x = np.array(x_scaled).transpose()\n",
    "        #print(x)\n",
    "        # Kategorien extrahieren und als Attribut speichern\n",
    "        y_values = list(set(y))                # Unique durch set()\n",
    "        self.categories = y_values             # Objektvariable setzen\n",
    "        # Verteilung auf Ausgabeneuronen (y)\n",
    "        y_spread = []\n",
    "        for y_i in y:\n",
    "            current = np.zeros(len(y_values))  # array 1D, Länge y-Werte mit Nullen\n",
    "            current[y_values.index(y_i)] = 1   # Setze Index\n",
    "            y_spread.append(current)           # Array Eintrag in Liste  \n",
    "        y_out = np.array(y_spread)\n",
    "        #print(y_out)\n",
    "        separator = int(test_ratio * len(x))   # Trenner für Testdaten\n",
    "        return x[:separator], y[:separator], x[separator:], y_out[separator:]\n",
    "\n",
    "    # Ein einzelner Trainingsdurchgang\n",
    "    # Attribute:\n",
    "    # - Eingabedaten als zweidimensionale Liste/Array\n",
    "    # - Zieldaten als auf Ausgabeneuronen verteilte Liste/Array\n",
    "    def train(self, inputs, targets):\n",
    "        # Daten ins richtige Format bringen\n",
    "        inputs = np.array(inputs, ndmin = 2).transpose()\n",
    "        targets = np.array(targets, ndmin = 2).transpose()\n",
    "        # Matrixmultiplikation: Gewichte versteckte Schicht * Eingabe\n",
    "        \n",
    "        self.training = {}\n",
    "        \n",
    "        self.training[\"hidden1_in\"] = np.dot(self.weights[\"input_to_hidden1\"], inputs)\n",
    "        # Aktivierungsfunktion anwenden\n",
    "        self.training[\"hidden1_out\"] = self.activation(self.training[\"hidden1_in\"])\n",
    "        \n",
    "        if len(self.hidden_neurons) > 1:\n",
    "            for i, _ in enumerate(self.hidden_neurons[1:]):\n",
    "                self.training[f\"hidden{i+2}_in\"]  = np.dot(self.weights[f\"hidden{i+1}_to_hidden{i+2}\"], self.training[\"hidden1_out\"])\n",
    "                self.training[f\"hidden{i+2}_out\"] = self.activation(self.training[f\"hidden{i+2}_in\"])\n",
    "        \n",
    "        # Matrixmultiplikation: Gewichte Ausgabeschicht * Ergebnis versteckt\n",
    "        output_in = np.dot(self.weights[f\"hidden{len(self.hidden_neurons)}_to_output\"], self.training[f\"hidden{len(self.hidden_neurons)}_out\"])\n",
    "        # Aktivierungsfunktion anwenden\n",
    "        output_out = self.activation(output_in)\n",
    "        \n",
    "        # Die Fehler berechnen\n",
    "        self.errors = {}\n",
    "        output_diff = targets - output_out\n",
    "        \n",
    "        self.errors[f\"hidden{len(self.hidden_neurons)}_diff\"] = np.dot(self.weights[f\"hidden{len(self.hidden_neurons)}_to_output\"].transpose(), output_diff)\n",
    "        \n",
    "        if len(self.hidden_neurons) > 1:\n",
    "            for i in range(len(self.hidden_neurons)-1)[::-1]:\n",
    "                self.errors[f\"hidden{i+1}_diff\"] = np.dot(self.weights[f\"hidden{i+1}_to_hidden{i+2}\"].transpose(), self.errors[f\"hidden{len(self.hidden_neurons)}_diff\"])   \n",
    "        \n",
    "        # Die Gewichte mit Lernrate * Fehler anpassen\n",
    "        self.weights[f\"hidden{len(self.hidden_neurons)}_to_output\"] += (\n",
    "            self.learning_rate *\n",
    "            np.dot(\n",
    "                (output_diff * output_out * (1.0 - output_out)),\n",
    "                self.training[f\"hidden{len(self.hidden_neurons)}_out\"].transpose()\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        if len(self.hidden_neurons) > 1:\n",
    "            for i, _ in enumerate(self.hidden_neurons[1:]):\n",
    "                self.weights[f\"hidden{i+1}_to_hidden{i+2}\"] += (\n",
    "                    self.learning_rate *\n",
    "                    np.dot(\n",
    "                        (self.errors[f\"hidden{i+2}_diff\"] * self.training[f\"hidden{i+2}_out\"] * (1.0 - self.training[f\"hidden{i+2}_out\"])),\n",
    "                        self.training[f\"hidden{i+1}_out\"].transpose()\n",
    "                    )\n",
    "                )\n",
    "        \n",
    "        self.weights[\"input_to_hidden1\"] += (\n",
    "            self.learning_rate *\n",
    "            np.dot(\n",
    "                (self.errors[\"hidden1_diff\"] * self.training[\"hidden1_out\"] * (1.0 - self.training[\"hidden1_out\"])),\n",
    "                inputs.transpose()\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Vorhersage für eine Reihe von Testdaten\n",
    "    # Attribute:\n",
    "    # - Eingabedaten als zweidimensionale Liste/Array\n",
    "    # - Vergleichsdaten (benannt, optional)\n",
    "    def predict(self, inputs, targets = None):\n",
    "        # Dieselben Schritte wie in train()\n",
    "        inputs = np.array(inputs, ndmin = 2).transpose()\n",
    "        self.training[\"hidden1_in\"] = np.dot(self.weights[\"input_to_hidden1\"], inputs)\n",
    "        # Aktivierungsfunktion anwenden\n",
    "        self.training[\"hidden1_out\"] = self.activation(self.training[\"hidden1_in\"])\n",
    "        \n",
    "        if len(self.hidden_neurons) > 1:\n",
    "            for i, _ in enumerate(self.hidden_neurons[1:]):\n",
    "                self.training[f\"hidden{i+2}_in\"]  = np.dot(self.weights[f\"hidden{i+1}_to_hidden{i+2}\"], self.training[\"hidden1_out\"])\n",
    "                self.training[f\"hidden{i+2}_out\"] = self.activation(self.training[f\"hidden{i+2}_in\"])\n",
    "        \n",
    "        # Matrixmultiplikation: Gewichte Ausgabeschicht * Ergebnis versteckt\n",
    "        output_in = np.dot(self.weights[f\"hidden{len(self.hidden_neurons)}_to_output\"], self.training[f\"hidden{len(self.hidden_neurons)}_out\"])\n",
    "        # Aktivierungsfunktion anwenden\n",
    "        output_out = self.activation(output_in)\n",
    "\n",
    "        # Ausgabewerte den Kategorien zuweisen\n",
    "        outputs = output_out.transpose()\n",
    "        result = []\n",
    "        for output in outputs:\n",
    "            result.append(\n",
    "                self.categories[list(output).index(max(output))]\n",
    "            )\n",
    "        # Wenn keine Zielwerte vorhanden, Ergebnisliste zurückgeben\n",
    "        #print('Ergebnis:',result)\n",
    "        #print('Vergleich:', targets)\n",
    "        if targets is None:\n",
    "            return result\n",
    "        # Ansonsten vergleichen und korrekte Vorhersagen zählen\n",
    "        correct = 0\n",
    "        for res, pred in zip(targets, result):\n",
    "            if res == pred:\n",
    "                correct += 1\n",
    "        percent = correct / len(result) * 100\n",
    "        return correct, percent\n",
    "\n",
    "\n",
    "def training_result(data, h_neurons = [6], learning_param = 0.03, repeats = 100):\n",
    "    # Hauptprogramm\n",
    "        network = NeuralNetwork(4, h_neurons, 3, learning_rate = learning_param)\n",
    "        x_test, y_test, x_train, y_train = network.prepare(data, test_ratio=0.2)\n",
    "        for i in range(repeats):\n",
    "            network.train(x_train, y_train)\n",
    "        correct, percent = network.predict(x_test, targets = y_test)\n",
    "        return percent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27004399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    if __name__ == '__main__':\n",
    "        with open('iris_nn.csv', 'r') as iris_file:\n",
    "            reader = csv.reader(iris_file, quoting=csv.QUOTE_NONNUMERIC)\n",
    "            irises = list(reader)\n",
    "        shuffle(irises)\n",
    "        return irises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c868dbc",
   "metadata": {},
   "source": [
    "## Funktion für automatisches Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50fe4c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from timeit import timeit as ti\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statistics import median, mean_arith, variance\n",
    "# from tqdm import tqdm\n",
    "from math import floor, ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0bdb1194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neurons_1_value</th>\n",
       "      <th>neurons_1_change</th>\n",
       "      <th>learning_rate_value</th>\n",
       "      <th>learning_rate_change</th>\n",
       "      <th>repeats_value</th>\n",
       "      <th>repeats_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>6.245211</td>\n",
       "      <td>0.0360</td>\n",
       "      <td>10.076628</td>\n",
       "      <td>120</td>\n",
       "      <td>9.310345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>6.245211</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>9.450830</td>\n",
       "      <td>0.0360</td>\n",
       "      <td>6.500639</td>\n",
       "      <td>120</td>\n",
       "      <td>7.918263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6.245211</td>\n",
       "      <td>0.0360</td>\n",
       "      <td>6.500639</td>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>6.500639</td>\n",
       "      <td>0.0432</td>\n",
       "      <td>5.734355</td>\n",
       "      <td>120</td>\n",
       "      <td>4.865900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>6.245211</td>\n",
       "      <td>0.0360</td>\n",
       "      <td>6.500639</td>\n",
       "      <td>120</td>\n",
       "      <td>4.865900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>6.436782</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>5.363985</td>\n",
       "      <td>100</td>\n",
       "      <td>5.312899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>6.245211</td>\n",
       "      <td>0.0360</td>\n",
       "      <td>6.500639</td>\n",
       "      <td>100</td>\n",
       "      <td>5.312899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6</td>\n",
       "      <td>8.033206</td>\n",
       "      <td>0.0432</td>\n",
       "      <td>5.619413</td>\n",
       "      <td>120</td>\n",
       "      <td>6.130268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>6.245211</td>\n",
       "      <td>0.0432</td>\n",
       "      <td>5.619413</td>\n",
       "      <td>100</td>\n",
       "      <td>5.312899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6</td>\n",
       "      <td>10.485313</td>\n",
       "      <td>0.0360</td>\n",
       "      <td>2.669221</td>\n",
       "      <td>83</td>\n",
       "      <td>5.887612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5</td>\n",
       "      <td>6.245211</td>\n",
       "      <td>0.0360</td>\n",
       "      <td>2.669221</td>\n",
       "      <td>100</td>\n",
       "      <td>5.312899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>8.224777</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>7.164751</td>\n",
       "      <td>120</td>\n",
       "      <td>4.968072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>6.245211</td>\n",
       "      <td>0.0360</td>\n",
       "      <td>2.669221</td>\n",
       "      <td>120</td>\n",
       "      <td>4.968072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6</td>\n",
       "      <td>4.137931</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>7.164751</td>\n",
       "      <td>100</td>\n",
       "      <td>3.895275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5</td>\n",
       "      <td>6.245211</td>\n",
       "      <td>0.0360</td>\n",
       "      <td>2.669221</td>\n",
       "      <td>100</td>\n",
       "      <td>3.895275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6</td>\n",
       "      <td>4.904215</td>\n",
       "      <td>0.0432</td>\n",
       "      <td>9.195402</td>\n",
       "      <td>83</td>\n",
       "      <td>4.546616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5</td>\n",
       "      <td>6.245211</td>\n",
       "      <td>0.0360</td>\n",
       "      <td>2.669221</td>\n",
       "      <td>83</td>\n",
       "      <td>4.546616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6</td>\n",
       "      <td>9.757344</td>\n",
       "      <td>0.0432</td>\n",
       "      <td>3.780332</td>\n",
       "      <td>100</td>\n",
       "      <td>9.757344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5</td>\n",
       "      <td>6.245211</td>\n",
       "      <td>0.0432</td>\n",
       "      <td>3.780332</td>\n",
       "      <td>83</td>\n",
       "      <td>4.546616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    neurons_1_value  neurons_1_change  learning_rate_value  \\\n",
       "0                 6          0.000000               0.0300   \n",
       "1                 5          6.245211               0.0360   \n",
       "2                 5          6.245211               0.0300   \n",
       "3                 6          9.450830               0.0360   \n",
       "4                 5          6.245211               0.0360   \n",
       "5                 6          6.500639               0.0432   \n",
       "6                 5          6.245211               0.0360   \n",
       "7                 4          6.436782               0.0300   \n",
       "8                 5          6.245211               0.0360   \n",
       "9                 6          8.033206               0.0432   \n",
       "10                5          6.245211               0.0432   \n",
       "11                6         10.485313               0.0360   \n",
       "12                5          6.245211               0.0360   \n",
       "13                4          8.224777               0.0300   \n",
       "14                5          6.245211               0.0360   \n",
       "15                6          4.137931               0.0300   \n",
       "16                5          6.245211               0.0360   \n",
       "17                6          4.904215               0.0432   \n",
       "18                5          6.245211               0.0360   \n",
       "19                6          9.757344               0.0432   \n",
       "20                5          6.245211               0.0432   \n",
       "\n",
       "    learning_rate_change  repeats_value  repeats_change  \n",
       "0               0.000000            100        0.000000  \n",
       "1              10.076628            120        9.310345  \n",
       "2               0.000000            100        0.000000  \n",
       "3               6.500639            120        7.918263  \n",
       "4               6.500639            100        0.000000  \n",
       "5               5.734355            120        4.865900  \n",
       "6               6.500639            120        4.865900  \n",
       "7               5.363985            100        5.312899  \n",
       "8               6.500639            100        5.312899  \n",
       "9               5.619413            120        6.130268  \n",
       "10              5.619413            100        5.312899  \n",
       "11              2.669221             83        5.887612  \n",
       "12              2.669221            100        5.312899  \n",
       "13              7.164751            120        4.968072  \n",
       "14              2.669221            120        4.968072  \n",
       "15              7.164751            100        3.895275  \n",
       "16              2.669221            100        3.895275  \n",
       "17              9.195402             83        4.546616  \n",
       "18              2.669221             83        4.546616  \n",
       "19              3.780332            100        9.757344  \n",
       "20              3.780332             83        4.546616  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [ get_data() for i in range(20) ]\n",
    "data = [ (round(training_result(d)),d) for d in data ]\n",
    "data = sorted(data)[len(data)//2][1]\n",
    "# data = get_data()\n",
    "\n",
    "h_neurons = [[(0,6)]]\n",
    "learning_rate = [(0,0.03)]\n",
    "repeats = [(0,100)]\n",
    "\n",
    "params = [*h_neurons, learning_rate, repeats]\n",
    "\n",
    "factor = 1.2\n",
    "list_size = 30\n",
    "for i in range(10):\n",
    "    results = []\n",
    "    for j,param in enumerate(params):\n",
    "        \n",
    "        one_changed_param = [*(list(zip(*params))[-1])]\n",
    "        if j != len(h_neurons):\n",
    "            param_increase = ceil(param[-1][1]*factor)\n",
    "            param_decrease = floor(param[-1][1]/factor)\n",
    "        else:\n",
    "            param_increase = round(param[-1][1]*factor,5)\n",
    "            param_decrease = round(param[-1][1]/factor,5)\n",
    "            \n",
    "        one_changed_param[j] = (0, param_increase)\n",
    "        increase_variance = variance([training_result(data, [ l[1] for l in one_changed_param[:len(h_neurons)] ], one_changed_param[-2][1], one_changed_param[-1][1]) for k in range(list_size)])\n",
    "        one_changed_param[j] = (0, param_decrease)\n",
    "        decrease_variance = variance([training_result(data, [ l[1] for l in one_changed_param[:len(h_neurons)] ], one_changed_param[-2][1], one_changed_param[-1][1]) for k in range(list_size)])\n",
    "        \n",
    "        if increase_variance < decrease_variance:\n",
    "            results.append((increase_variance, param_increase))\n",
    "        else:\n",
    "            results.append((decrease_variance, param_decrease))\n",
    "    \n",
    "    for k,result in enumerate(results):\n",
    "        params[k].append(result)\n",
    "        \n",
    "    min_variance = min([k for k,l in results])\n",
    "    \n",
    "    for k,result in enumerate(results):\n",
    "        if min_variance == result[0]:\n",
    "            params[k].append(result)\n",
    "        else:\n",
    "            params[k].append(params[k][-2])\n",
    "            \n",
    "df = pd.DataFrame()\n",
    "\n",
    "for i in range(len(h_neurons)):\n",
    "    df[f\"neurons_{i+1}_value\"] = [ k for j,k in params[i]]\n",
    "    df[f\"neurons_{i+1}_change\"] = [ j for j,k in params[i]]\n",
    "\n",
    "df[\"learning_rate_value\"] = [ k for j,k in params[-2]]\n",
    "df[\"learning_rate_change\"] = [ j for j,k in params[-2]]\n",
    "df[\"repeats_value\"] = [ k for j,k in params[-1]]\n",
    "df[\"repeats_change\"] = [ j for j,k in params[-1]]\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
